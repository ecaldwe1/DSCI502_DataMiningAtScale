{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week7_IntroToStreams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPByzRP/EghHJhDH77SwyuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecaldwe1/DSCI502_DataMiningAtScale/blob/main/Week7_IntroToStreams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streams\n",
        "\n",
        "By a stream, we will refer to data which arrives at a variable rate. In the event that the volume of data is extremely large then it will not be possible to capture all the data.\n",
        "\n",
        "Tasks such as calculating simple statistics (eg frequencies) becomes challenging. We will generally suppose that it is not possible to store the entire stream in memory.\n",
        "\n",
        "Determining exact values is sometimes not possible. In this situation, we will have to be content with an estimate that will have a significant probability of being close to the true value.\n",
        "\n",
        "In the next few lectures we will be examining certain queries that can be performed on streams. We will rely on being able to produce good approximations to the true answer. \n",
        "\n",
        "To begin, we will consider the possibility of sampling the stream."
      ],
      "metadata": {
        "id": "7-djxD3UwJT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Data in a Stream\n",
        "\n",
        "This example is taken from Mining of Massive Data Sets.\n",
        "\n",
        "Suppose that we have a stream of which the elements are userids and a search query $(userId, query)$ . We will consider sampling the stream. For concreteness, let's suppose that we will sample $\\frac{1}{10}$ of the stream values.\n",
        "\n",
        "For each query, we will choose a number from $0$ to $9$ at random and preserve the query if it is $0$. \n",
        "\n",
        "If the number of queries per user is large, the law of large numbers will imply that roughly one-tenth of a users queries will be preserved. \n",
        "\n",
        "We will now consider a query which asks what portion of a users queries are duplicates.\n",
        "\n",
        "Let us suppose that for a given user there are $d$ queries which are made only once and there are $t$ queries which are made two times. This corresponds to $d+2t$ stream elements. \n",
        "\n",
        "We will first calculate the number of queries which show up only once in the sample. For the $d$ queries which are made only once, this is easy. There will be about $\\frac{d}{10}$ of them in the sample.\n",
        "\n",
        "For a query which was made twice, there is a probability of $\\frac{9}{50}$ that this will appear once in the sample and $\\frac{1}{100}$ that it will appear twice. \n",
        "\n",
        "This implies that the sample will have approximately $\\frac{d}{10} + \\frac{9t}{50}$ queries which appear once and $\\frac{t}{100}$ queries which are duplicated. \n",
        "\n",
        "We see that there will be approximately \n",
        "$$\\frac{d}{10}+\\frac{9t}{50}+\\frac{t}{100} = \\frac{10d+19t}{100}$$\n",
        "queries in the sample.\n",
        "\n",
        "It follows that using the sample, we will estimate the probability that a query is duplicated to be\n",
        "$$ \\frac{t/100}{(10d+19t)/100} = \\frac{t}{10d+19t}$$\n",
        "\n",
        "The actual probability of a query appearing more than once in this case is\n",
        "$$ \\frac{t}{d+t}$$\n",
        "\n",
        "We see that the approach here is not capable of giving a sharp approximation."
      ],
      "metadata": {
        "id": "0c-w8xV0xcN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternate Approach\n",
        "\n",
        "In order to avoid this issue, we will need to take all of the queries from a sample of users. \n",
        "\n",
        "Suppose we have $B$ buckets, numbered from $0$ to $B-1$. \n",
        "\n",
        "We can no longer rely on random number generation because we would need the same value for a user, each time we see that user. Instead, we will make use of a hash function, $h$. When we see a user, we can hash the user id and we know we will get the same thing each time. (Assuming that the hash function is good and will evenly distribute user ids.)\n",
        "\n",
        "We can specify some initial value $x$ which is between $0$ and $B-1$ We assume that a user hashes to one of the $B$ buckets. We will save all of the user data for those users which hash to a bucket $0$ to $x$. \n",
        "\n",
        "In the beginning, $x$ could be $B-1$ in order to have a large enough sample, but eventually that approach will run out of memory and the value of $x$ will need to decrease. \n",
        "\n"
      ],
      "metadata": {
        "id": "XGRVTepclAAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXQxS8ysvtd6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}